{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK (https://www.nltk.org/) 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rose/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to /Users/rose/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rose/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rose/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/rose/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 문장 토큰화(sentence tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 주어진 텍스트를 문장 단위로 토큰화. 주로 . ! ? 등을 이용\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "# 영어가 아닌 다른 언어 사용 시 사전 학습된 모델을 지정해 불러올 수 있음\n",
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "                    Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "                    Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트 마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트 마이닝 클래스를 시작해봅시다!\"\n",
    "print(sent_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 단어 토큰화 (word tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 주어진 텍스트를 단어 단위로 토큰화.\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokenize vs. WordPunctTokenizer : 토크나이저가 서로 다른 알고리즘에 기반하기 때문에 차이 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트', '마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "# 한국어는 형태소 분석 등 다양한 word segmentation 방법 활용해야 함 : KoNLPy\n",
    "print(word_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 정규표현식을 이용한 토큰화(https://wikidocs.net/4308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n",
      "['3', '7', '5', '9']\n",
      "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']\n",
      "['_', '__', '___']\n",
      "['How', 'are', 'you', 'boy']\n",
      "['oo', 'oooo', 'oooo', 'ooo']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.findall('[abc]', 'How are you, boy?'))\n",
    "print(re.findall('[0-9]', '3a7b5c9d'))\n",
    "print(re.findall('[\\w]', \"3a 7b_ '.^&5c9d\"))\n",
    "print(re.findall('[_]+', 'a_b, c__d, e___f'))\n",
    "print(re.findall('[\\w]+', 'How are you, boy?'))\n",
    "print(re.findall('[o]{2,4}', 'oh, hoow are yoooou, boooooooy?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 노이즈와 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())  # word_tokenize로 토큰화\n",
    "\n",
    "# stopwords를 제외한 단어들만으로 list를 생성\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down', 'each', 'who', 'of', 'why', 'only', 're', \"shouldn't\", 'have', 'll', 'are', \"hasn't\", 'whom', 'no', 'ma', 'into', 'at', 'ourselves', 'were', 'she', 'this', \"hadn't\", 'weren', 'by', 'theirs', 'couldn', 'my', 'own', 'so', 'than', 'now', 'ours', 'do', 'its', 'a', 'but', 'doesn', 'we', 'that', \"haven't\", 'y', \"you'll\", 'themselves', 'shan', 'itself', \"you've\", 'some', 'has', 'did', 'if', 'yours', 'up', 'won', 'against', 'doing', 'for', 've', 'had', 'after', 'hasn', 'about', \"didn't\", 'myself', 'yourselves', 'nor', 'needn', \"needn't\", 'was', 'wouldn', 'over', 'both', 'out', 'haven', 'or', 'too', 'didn', 'off', 'more', 'been', 'until', 'very', 'mightn', 'you', 'they', 'having', \"aren't\", 'it', 'these', 'hers', 'am', \"you're\", 'i', 'here', \"mightn't\", 'herself', \"couldn't\", 'which', 'be', 'how', 'such', 'once', 'where', 'shouldn', 'under', 'o', \"she's\", 'your', \"wasn't\", 'him', 'same', 'an', 'before', 'further', 'mustn', 'then', 'because', \"it's\", 'other', 'between', \"shan't\", \"don't\", 'few', 'he', 'those', 'there', 'all', 'with', 'her', 'through', 'me', 'what', \"you'd\", 'their', \"won't\", 'not', 'on', 'is', 'our', 'can', 'm', 'again', 'will', 'from', 'hadn', 'while', 'them', 'isn', \"isn't\", \"should've\", 'ain', \"doesn't\", 'most', \"mustn't\", 'being', 'should', 'just', \"that'll\", 'does', 'd', 'himself', \"wouldn't\", 's', 'aren', 't', 'and', 'don', 'his', 'above', 'in', 'wasn', 'as', 'any', 'yourself', 'the', 'during', 'below', \"weren't\", 'to', 'when'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "# 자신만의 stopwords를 만들고 이용(한글 처리에 유용, 리스트로 정의)\n",
    "my_stopword = ['i', 'go', 'to']\n",
    "result = [word for word in tokens if word not in my_stopword]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para)\n",
    "print(tokens)\n",
    "result = [stemmer.stem(token) for token in tokens]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))  # 품사 지정\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison of stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result:  believ\n",
      "lemmatizing result:  belief\n",
      "lemmatizing result:  believe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result: ', stemmer.stem('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 품사 태깅(Part-of-Speech Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 품사의 이해 : 명사, 대명사, 수사, 조사, 동사, 형용사, 관형사, 부사, 감탄사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 NLTK를 이용한 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(nltk.pos_tag(tokens))\n",
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
     ]
    }
   ],
   "source": [
    "# 단어에 품사 정보를 추가해 구분하면 성능 좋음\n",
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
    "print(words_with_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 한글 형태소 분석과 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy 설치(https://konlpy.org/ko/latest/install/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy의 Twitter 클래스 임포트(형태소 분석기)\n",
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소:  ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
      "\n",
      "명사:  ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
      "\n",
      "품사 태깅 결과:  [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "print('형태소: ', t.morphs(sentence))\n",
    "print()\n",
    "print('명사: ', t.nouns(sentence))\n",
    "print()\n",
    "print('품사 태깅 결과: ', t.pos(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
